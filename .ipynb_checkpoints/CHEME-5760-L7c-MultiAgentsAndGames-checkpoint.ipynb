{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9a7180a-6b4c-4a97-9d13-35647c2f094c",
   "metadata": {},
   "source": [
    "# Lab 7c: Simple Games - Traveler‚Äôs Dilemma\n",
    "The Traveler‚Äôs Dilemma is a non-cooperative __non-zero sum game__ that involves an airline losing two identical suitcases belonging to two different travelers. The airline requests that the travelers report the value of their suitcases, which must fall in the range of `2 USD` and `100 USD`, in increments of $\\pm$ `1 USD`.\n",
    "\n",
    "__Rules__:\n",
    "* If both travelers report the same value, they receive that value as a reward. \n",
    "* However, if the travelers report different values, the traveler with the lower value receives their reported value plus an additional `2 USD`, while the traveler with the higher value receives the lower value minus `2 USD`. \n",
    "\n",
    "The reward function is determined as follows:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "R_{i}(a_{i},a_{-i}) = \n",
    "\\begin{cases}\n",
    "a_{i} & \\text{if } a_{i} = a_{-i} \\\\\n",
    "a_{i} + 2 & \\text{if } a_{i} < a_{-i} \\\\\n",
    "a_{-1} - 2 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "Most people tend to put down between `97 USD` and `100 USD`. However, somewhat counter-intuitively, there is a unique [Nash equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium) of only `2 USD`.\n",
    "\n",
    "### Learning objectives\n",
    "The objective of `Lab 7c` is to familiarize students with the [Traveler‚Äôs Dilemma problem](https://en.wikipedia.org/wiki/Traveler%27s_dilemma), the solution of the problem using iterative refinement and the concept of [Nash Equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium).\n",
    "\n",
    "* The [Traveler‚Äôs Dilemma problem](https://en.wikipedia.org/wiki/Traveler%27s_dilemma) was first posed by [Kaushik Basu](https://en.wikipedia.org/wiki/Kaushik_Basu), a Cornell professor and former Chief Economist of the World Bank (2012 - 2016). For more information on this problem (beyond what is described here), check out this [article](https://www.academia.edu/56129718/The_Travelers_Dilemma). This problem (as we shall see) has a [Nash Equilibrium solution](https://en.wikipedia.org/wiki/Nash_equilibrium) of `2 USD`.\n",
    "* In [Nash Equilibrium](https://en.wikipedia.org/wiki/Nash_equilibrium) each player is assumed to know the equilibrium strategies of the other players, and no single player can gain by changing only their strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cfaffc-a639-4965-b29a-b20bad0e0cb8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f40ab7b7-08b4-46b4-80ec-8c848beb412b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLQuantitativeFinancePackage.jl.git`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Labs-F23/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Labs-F23/Manifest.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLDecisionsPackage.jl.git`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Labs-F23/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Labs-F23/Manifest.toml`\n",
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Desktop/julia_work/CHEME-5760-Labs-F23`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLDecisionsPackage.jl.git`\n",
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m git-repo `https://github.com/varnerlab/VLQuantitativeFinancePackage.jl.git`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Labs-F23/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/julia_work/CHEME-5760-Labs-F23/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115e5b2-0b51-411f-a794-9d8ac862725c",
   "metadata": {},
   "source": [
    "## Task 1: Build a `MySimpleGameModel` instance\n",
    "\n",
    "#### Model\n",
    "Let's begin `Lab 7c` by constructing a model of the game, which is an instance of the `MySimpleGameModel` type:\n",
    "\n",
    "```julia\n",
    "# The game model holds values (and functions) that are useful to evaluate the game\n",
    "mutable struct MySimpleGameModel <: AbstractGameModel\n",
    "\n",
    "    # data -\n",
    "    Œ≥   # discount factor -\n",
    "    ‚Ñê   # set of players -\n",
    "    ùíú   # joint action space\n",
    "    R   # joint reward function\n",
    "\n",
    "    # # constructor -\n",
    "    MySimpleGameModel() = new();\n",
    "end\n",
    "```\n",
    "\n",
    "Instances of the `MySimpleGameModel` type have the following fields:\n",
    "\n",
    "* The `Œ≥::Float64` field holds the discount factor for the game (the weight of current versus future rewards, not used in this game).\n",
    "* The `‚Ñê::Array{Int64,1}` field holds the list of players, in our case `{1,2}`\n",
    "\n",
    "\n",
    "#### Build\n",
    "We build a game model by passing the type of game we want to construct, in this case a `MyTravelersProblem`, into a the `build(...)` method:\n",
    "\n",
    "```julia\n",
    "function build(simpleGame::Type{MyTravelersProblem})\n",
    "\n",
    "    # build an empty model -\n",
    "    model = MySimpleGameModel();\n",
    "    \n",
    "    # populate the model -\n",
    "    model.Œ≥ = 0.9;\n",
    "    model.‚Ñê = vec(collect(1:n_agents(simpleGame)))\n",
    "    model.ùíú = [ordered_actions(simpleGame, i) for i in 1:n_agents(simpleGame)]\n",
    "    model.R = (a) -> joint_reward(simpleGame, a)\n",
    "\n",
    "    # return the model -\n",
    "    return model;\n",
    "end\n",
    "```\n",
    "\n",
    "The `build(...)` method constructs an empty model, then populates the model with the required data. Save your instance of the game model in the `mysimplemodel` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93f447f6-864e-455d-a702-e892534edd16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mysimplemodel = build(MyTravelersProblem);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30833666-e0e7-4223-9bde-3d6641b6f897",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2: Compute the iterated best policy for the Traveler‚Äôs Dilemma problem\n",
    "\n",
    "The iterated best policy is computed using the `solve(...)` method, given by:\n",
    "\n",
    "```julia\n",
    "function solve(M::MyIteratedBestResponsePolicy, ùí´::MySimpleGameModel)\n",
    "    œÄ = M.œÄ\n",
    "    for _ in 1:M.k_max\n",
    "        œÄ = [best_response_policy(ùí´, œÄ, i) for i in ùí´.‚Ñê]\n",
    "    end\n",
    "    return œÄ\n",
    "end\n",
    "```\n",
    "\n",
    "The `solve(...)` method takes the `MyIteratedBestResponsePolicy` and `MySimpleGameModel` instances and returns the best response policy for the Traveler‚Äôs Dilemma problem, computed by iterative refinement. The updates continue for `k_max` iterations, where during each iteration:\n",
    "\n",
    "* The joint policy $\\pi$ is updated for each player $i\\in\\left\\{1,2\\right\\}$ using an [array comprehension](https://docs.julialang.org/en/v1/manual/arrays/#man-comprehensions) operation. The update calls the `best_response_policy(...)` function, which returns the deterministic best policy.\n",
    "* After `k_max` updates, the refined joint policy $\\pi$ is returned to the caller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d047b3-252c-47cf-afa9-cdc5dd637a5e",
   "metadata": {},
   "source": [
    "### Initialize a uniformly random policy\n",
    "To construct an initial policy, we use the `build(...)` method to build and initialize an `MyIteratedBestResponsePolicy` instance. The `build(...)` method is given by:\n",
    "\n",
    "```julia\n",
    "function build(ùí´::MySimpleGameModel, k_max)\n",
    "    œÄ = [MySimpleGamePolicy(ai => 1.0 for ai in ùíúi) for ùíúi in ùí´.ùíú]\n",
    "    return MyIteratedBestResponsePolicy(k_max, œÄ)\n",
    "end\n",
    "```\n",
    "\n",
    "The `build(...)` method takes a `MySimpleGameModel` instance, and a value for the `k_max` parameter and returns a `MyIteratedBestResponsePolicy` with a uniform policy, which is stored in the `initial_iterated_policy` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6cdd94a-31fb-4874-8e3a-20d8bd5ad80c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_iterated_policy = build(mysimplemodel, 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8844364e-0674-479c-a7f6-3afd5f1ebc74",
   "metadata": {},
   "source": [
    "### Solve\n",
    "Now that we have a `initial_iterated_policy`, and the `mysimplemodel`, we can solve the problem using the `solve(...)` method shown above. The `solve(...)` method iteratively updates the initial policy and returns the `updated_policy` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf5bfc31-8325-47e0-8933-ed0ed2009218",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{MySimpleGamePolicy}:\n",
       " MySimpleGamePolicy(Dict(2 => 1.0))\n",
       " MySimpleGamePolicy(Dict(2 => 1.0))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_policy = VLDecisionsPackage.solve(initial_iterated_policy, mysimplemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b9867-a893-4833-a0d9-1a5667a8ed84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
